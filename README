------------------------------------
--- Distributed Logical Database ---
---                              ---
--- L. Thiry, 30/05/2017         ---
------------------------------------

--- Introduction ---
The aim of this project is to propose efficient (functional) programs 
able to query a large number of data.

As an illustration, the "titanic.csv" database is an array composed with 
approximatively 8000 elements with the 891 passengers whose name, sex, 
status (i.e. has survived), etc. is known. 

The program considered takes a query such as "(?X Sex female) and 
(?X Survived 0)", i.e. who are the women ?X that survived to the accident, 
and return the various values of ?X.

In the first step, the database and the queries are modeled as graphs, 
and the program must find all the possible morphisms/matchings between 
a query and the base.

The performance correspond to the time required to get the result, and 
depends mainly on the data structures considered. Optimizations can 
then be considered in the next steps by changing data structures (and 
consequently programs) to improve performances.

After having presented to basic program answer::Graph->Result, X 
optimizations are presented and discussed with:
1) the property of answer to be a monoidal morphism what can serve to 
split computations in various computers or multiprocessors architectures 
- i.e. answer (g1Ug2)=(answer(g1))U(answer(g2)),
2) the size reduction of the input data based on a isomorphism g=_t g' 
and answer' = answer.t
3) the size reduction of the output data (usefull for exchanging data in 
distributed computers) based on another isomorphism r=_s r' and
answer'' = s.answer
4) an isomorphism (Q->R =_u [(Q,R)])used to memorize previous results 
and faster computations.

The benefits/limitations of each one are discussed and related to the 
performances of the MongoDB industrial standard.

--- Step 1 ---
The graph and queries are encoded as lists of edges, and an edge is 
a triple of strings (subject, predicate, object).

type Edge    = (String,String,String)
type Graph   = [Edge]
type Pattern = Graph

The program is then defined by the following functions. The context 
is used to store the values of the variables found in a query (e.g. "?X"), 
and the result is essentially a list of contexts.

match1 :: String -> String -> Context -> [Context]
With for instance:
match1 "?x" "y" []           == [[("?x","y")]]
match1 "?x" "y" [("?x","y")] == [[("?x","y")]]
match1 "?x" "y" [("?x","z")] == []

match3 String^3 -> String^3 -> Context -> [Context]
With for instance:
match3 ("?x","y","z") ("a","y","z") [] == [[("?x","a")]]

matchn :: String^3 -> Graph -> Context -> [Context]
With for instance:
matchn ("?x","y","z") [("a","y","z"),("b","y","z")] []
== [[("?x","a"),("?x","b")]]

answer :: Pattern -> Graph -> Context -> [Context]
With for instance:
answer [("?x","b","?y"),("?y","d","e")] [("b","d","e"),("a","b","c")] []
== [[("?x","a"),("?y","c")]]

And the context is defined by:
type Context = [(String,String)]
has :: String -> Context -> Bool
get :: String -> Context -> String
put :: String -> String -> Context -> Context

The main application (simplified) is then defined by:

main  = do
 args <- getArgs
 db   <- readCSV "titanic.csv"
 let q = query (args!!0) -- with a parser, query :: String -> Pattern
 print (answer q db)

And a sample usage in standalone mode is:
./Main "(?x Survived 1) and (?x Sex male)"

In distributed mode:
./Main "slave" "titanic.dhs" "9000"    # in the various servers
./Main "server" "(?x Survived 1)" # in the client(s)
# with a configuration file for the client config.txt containing for 
instance:
[("ip1",9000),("ip2",9000)]

The performances are then obtained by using the "time" command, e.g.
time(./Main "master" "(?X Survived 0) and (?X Sex female)")

real	0m0.515s
user	0m0.016s
sys		0m0.012s

Another approach is proposed to compute time directly inside a programs.

--- Performances ---
By assuming that the program returns the good results, the following 
consideration is: how long does it take ?

The answer to this question consists in getting time before then after 
a computation and returning the difference (or equivalently by using 
the time command in a unix shell).

The problem is here that Haskell is a lazy language and it only makes 
necessary computations: i.e. (answer pattern graph) will be effectively 
computed if the result is used. The solution is to use the "evaluate" 
function that force its argument to be evaluated (call by value).

Thus, the performance measurement will be obtained with:

performance fx = do
 t1 <- getCurrentTime
 evaluate fx
 t2 <- getCurrentTime
 return (diffUTCTime t2 t1)

Example of values obtained when changing the size of database or 
the query are:

Size \ Query |    q1    |    q2    |    q3
0.1%         | 0.00084s | 0.00047s | 0.00044s
1%           | 0.00891s | 0.00389s | 0.00637s
10%          | 0.04412s | 0.04378s | 0.04432s
50%          | 0.17314s | 0.18080s | 0.17075s
100%         | 0.34306s | 0.36010s | 0.35782s

With the following queries:
q1 = (?X Sex male)
q2 = (?X Sex male) and (?X Survived 1)
q3 = (?X ?Y ?Z)

As a comparison, the performances obtained by using MongoDB with a 
single collection for the edges are:
q1 = 0.00027s
q3 = 0.00024s

The jointure (q2) can not be easily expressed and is omitted.

An analysis of the results shows that:
1) the time required is approximatively proportionnal to the size of 
the database
2) the query has no real impact in the performance
3) the program is really slower than MongoDB (1000x faster)

Notes. Some improvements have been considered with the use of: 1) the tail 
recursion and accumulators (see DBacc), 2) the cut-fusion property, and 
2) the optimization flags (-O, -f*) and capabilities of the ghc compiler. 
But, no significant effects have been noticed.

--- Optimizations ---
Now, other data structures can be considered and lead to other 
but similar programs. For instance, graphs and contexts can be encoded 
as follow:

type Graph'  = (Graph,Graph)
type Graph'' = [(String,[(String,String)])]
type Context' = ([String],[[String]])
...

Equality between the various representations of a data structure can be 
formalized by isomorphisms. For instance, by considering a function that 
split a list in two and the concatenation of lists, a property is 
that for all lists l, concatenate(split(l)) = l.

split       :: Graph  -> Graph'
concatenate :: Graph' -> Graph

This property can be used to get a new version of the answer program:

answer' pat grp ctx = 
 let (grp1,grp2) = split(grp)
     r1 = answer pat grp1 ctx
     r2 = answer pat grp2 ctx
  in concatenate(r1,r2)

The split function has next to be improved according the queries 
considered. Indeed, if grp1 has an edge (a Sex female) and grp2 
an edge (a Survived 0) then answer will returns [(?X,a)] to the 
query "(?X Sex female) and (?X Survived 0)" but answer' will return 
nothing.

However, the preceding program is interesting to introduce concurrency 
and distribution. Indeed, (r1,r2) can be computed separatly and by two 
different processors (eventually placed in a network). By using this 
property, computation time and performances can be improved.

--- Step 2: concurrency ---
A simple implementation of concurrency/distribution is given below. 
The answer program is represented by slave processes/nodes using a 
part of the whole database. The master program then dispatches a query 
to a list of nodes and merges the results. The sample programs 
shows how to use these elements.

slave db port = withSocketsDo $ do 
 sock <- listenOn $ PortNumber port
 forever $ do
  (handle, host, port) <- accept sock
  query  <- hGetLine handle
  let pat = read query :: Graph
  let res = answer pat db []
  hPutStrLn handle (show res)
  hFlush handle
  hClose handle

master hosts query = withSocketsDo $ do
 let nodes = map (\h -> connectTo (fst h) (PortNumber (snd h))) hosts
 res <- sequence (map (\n -> transmit n q) nodes)
 let r = concat res
 print r
  where
   transmit node query = do
    handle <- node
    hPutStrLn handle query
    hFlush handle
    rep <- hGetLine handle
    let r = read rep :: Result
    hClose h
    return r

readDB :: Filename -> IO Graph
query  :: String   -> Pattern

sample = do
 let q = query "(?X Sex female)"
 db1 <- readDB "titanic-a"
 db2 <- readDB "titanic-b"
 forkIO (slave db1 9000)
 forkIO (slave db2 9001)
 threadDelay 10000
 master [("localhost",9000),("localhost",9001)] (show q)

The performances for an architecture with two slaves are now:

Size \ Query |    q1    |    q2    |    q3
100%         | 0.01305s | 0.00182s | 0.46910s

An analysis of the results shows that:
1) the is obtained 30 times faster for q1, and 300 times faster for d2. 
This can be explained by noticing that the result of q2 is a subset 
from the one of q1.
2) the query, or more precisely the size of the result, has a real 
impact in the performance as illustrated by q3 that returns the whole db 
(what requires an important transmission time), 
3) the program is now 10-100 times slower than MongoDB.

So, the performances of the program depends on the size of the 
result(s) - what can be generalized to the standalone program, and an 
possible optimization is presented at step 4.

--- Step 3: (re)arranging data  ---
As shown in step 1, the performances are related to the size of the 
database and changing its structure can leads to faster programs.
In particular, an invertible transformation trf::Graph->Graph'' can be 
defined. Thus, if the database/graph has approximatively 8000 elements 
corresponding to the edges then trf(graph)/graph'' has 800 elements 
representing the passagers and the properties of each one.

Next, the program answer is adapted to use this new data structure:

answer'' :: Pattern -> Graph'' -> Context -> Result

And the performances obtained are in a single application:

Size \ Query |    q1    |    q2    |    q3
50%          | 0.14353s | 0.15395s | 0.13411s
100%         | 0.28054s | 0.27429s | 0.27274s

And in a distributed application with 2 slaves:

Size \ Query |    q1    |    q2    |    q3
100%         | 0.01044s | 0.00177s | 0.43470s

An analysis of the results shows that:
1) the gain obtained is 17% for q1, 25% for q2 and 22% for q3 in a 
single application, 
2) the gain is 23% for q1, 5% for q2, and 6% for q3 in the distributed 
case. 
This confirms that 1) the choice of a good data structure can really 
improve the performance of a program (approx. 20% in the single 
application), and 2) the distributed case is influenced by other 
parameters (approx. 10%) such as the data structure used for the results.

--- Step 4: (re)arranging results  ---
An invertible transformation trf'::Context->Context' can be 
defined. Thus, if the results set has n (key,value) elements,
corresponding to the possible values of the variables in a query, then 
trf'(result)/result' has approximatively n/2 (key,values) elements.

E.g. trf' [(x,v1),(x,v2),(x,v3)] == [(x,[v1,v2,v3])]

The program answer is adapted to use this new data structure and the 
performances are (See DB3 and Main3):

Mode \ Query |    q1    |    q2    |    q3
Single       | 0.35186s | 1.33218s | 0.36304s
Distributed  | 0.00365s | 0.00071s | 0.16356s

An analysis of the results shows that:
1) the performances of new version of the application are similar to 
the original application for q1 and q3, but 3x slower for q2. The 
structure of the result returned as a deep impact in the algorithm that 
involved here extra treatments.
2) the distributed version of the new application is, except for q3, 
10x more faster than the original one. This can be explained by the 
size reduction of the result.

--- Step 5: memorization  ---
Another optimization consists in replacing computations by memorizations, 
i.e. a function f:x->y is associated to a association list l=[(x,y)], and 
f'_l(x) = (l(x),l) if x:dom(l), = (f(x),l U {(x,f(x))}) otherwise.
Thus, already performed computations are stored and used before computing 
a result that will be memorized, etc.

The performances then become:

Memo \ Query |    q1    |    q2    |    q3
Without      | 0.36690s | 0.28337s | 0.00053s
With         | 0.00016s | 0.00019s | 0.00013s

The results shows that:
1) the second time a value is computed requires approximatively 0.00015s 
what is similar to the performance of MongoDB, 
2) each computations can improve the next computations - for instance, 
the performances for q2 is reduced for the original version of the 
program and can be explained by the fact that q1 appears as a part of q2,
3) as a complement to the preceding remark, the performance for q3 can 
not be explained despite many checks of the code.
 
Thus this final version of the program is the consumming the less time 
... but the more memory.

--- Step 6: Avoiding strings ---
As a final example of optimization, by susbstituting strings by 
integers/references in the graph leads to the performances below what 
is 5-10x slower then MongoDB.

 \ Query |    q1    |    q2    |    q3
         | 0.00063s | 0.00085s | 0.00046s

A alternative is to use ByteString, i.e. a more efficient representation 
of strings (=[Char])

--- Conclusion ---
An open source software for distributed logical databases more efficient 
than MongoDB and freely available at https://github.com/thiry/DComp.git
It include a command line utility to start slave programs that can be 
eventually multiple and distant (each one is responsible of a part of 
the whole database), or to start a master program that interprets client 
queries - a concrete and human readable syntax is proposed to facilitate 
its use.

There are other capabilities offered by the system such as graph 
rewriting that are currently studied and will be presented in the future.


